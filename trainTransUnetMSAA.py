

import os
import random
import numpy as np
import torch
import torch.optim as optim
from torch import nn
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from transformers.commands import train
from tensorflow.keras.optimizers import Adam
# from unet.unet3 import UNet3D
import torch.nn.functional as F
from utils import DataGenerator
import configs as configs
from TransUnetMSAA import VisionTransformer
import time
from tqdm import tqdm
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay  # æ·»åŠ å¯¼å…¥
import matplotlib.pyplot as plt
import seaborn as sns

seed = 12345
torch.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)

torch.cuda.empty_cache()
torch.backends.cudnn.benchmark = False
# å®šä¹‰ä¿å­˜æ£€æŸ¥ç‚¹çš„å‡½æ•°ï¼Œä¿å­˜ä¸º.pthæ–‡ä»¶
def save_checkpoint(state, filename="SeismicTUMSAAæŸå¤±ä¼˜åŒ–.pth"):
    torch.save(state, filename)

# åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
config = configs.get_r50_b16_config()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = VisionTransformer(config, img_size=128, num_classes=21843, zero_head=False, vis=False).to(device)

# # # # # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡
# # pretrained_path = r"/root/autodl-tmp/modelNoiseTU-MSAA/46Seismic+NoiseNoisecheckpointTUMSAA.70.pth" # æ›¿æ¢ä¸ºå®é™…çš„é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
# # state_dict = torch.load(pretrained_path)
# # # æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹æ˜¯å¦åŒ…å«ä¸å¿…è¦çš„é”®
# # if "state_dict" in state_dict:
# #     state_dict = state_dict["state_dict"]
# # model.load_state_dict(state_dict)

# criterion = nn.CrossEntropyLoss()
# optimizer = optim.Adam(model.parameters(), lr=1e-4)

# ä¼˜åŒ–åçš„æŸå¤±å‡½æ•°æ–¹æ¡ˆ
# DiceæŸå¤±å®ç°

class DiceLoss(nn.Module):
    def __init__(self, smooth=1e-6):
        super().__init__()
        self.smooth = smooth
        
    def forward(self, prediction, target):
        prediction = torch.sigmoid(prediction)
        intersection = (prediction * target).sum()
        union = prediction.sum() + target.sum()
        dice = (2. * intersection + self.smooth) / (union + self.smooth)
        return 1 - dice
class CaveOptimizedLoss(nn.Module):
    def __init__(self, alpha=0.7, beta=0.3, cave_class_idx=1):
        super().__init__()
        self.alpha = alpha  # æº¶æ´æŸå¤±æƒé‡
        self.beta = beta    # å…¶ä»–æŸå¤±æƒé‡
        self.cave_class_idx = cave_class_idx  # æº¶æ´ç±»åˆ«ç´¢å¼•
        self.ce_loss = nn.CrossEntropyLoss()
        self.dice_loss = DiceLoss()
        
    def forward(self, outputs, targets):
        # ä¸»äº¤å‰ç†µæŸå¤±
        ce_loss = self.ce_loss(outputs, targets)
        
        # æº¶æ´ç‰¹å¼‚æ€§æŸå¤±
        cave_mask = (targets == self.cave_class_idx).float()
        cave_dice_loss = self.dice_loss(outputs[:, self.cave_class_idx], cave_mask)
        
        # ç»„åˆæŸå¤±
        total_loss = self.alpha * cave_dice_loss + self.beta * ce_loss
        return total_loss

# ä½¿ç”¨ä¼˜åŒ–åçš„æŸå¤±å‡½æ•°
criterion = CaveOptimizedLoss(alpha=0.3, beta=0.7, cave_class_idx=2)
# ä¼˜åŒ–å™¨é…ç½®
optimizer = optim.Adam(model.parameters(), lr=1e-4)

scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=20, min_lr=1e-8)

# æ•°æ®è·¯å¾„
sxpath = "/root/autodl-fs/ChannelKarst/seismic"
kxpath = "/root/autodl-fs/ChannelKarst/label"
n1, n2, n3 = 256, 256, 256
tdata_ids = list(range(1, 121))
vdata_ids = list(range(121, 141))
params = {'dim': (n1, n2, n3), 'n_channels': 1}
train_dataset = DataGenerator(dpath=sxpath, fpath=kxpath, data_IDs=tdata_ids, **params)
valid_dataset = DataGenerator(dpath=sxpath, fpath=kxpath, data_IDs=vdata_ids, **params)
train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False)

# TensorBoardæ—¥å¿—è®°å½•
writer = SummaryWriter(log_dir='./log')
# è®­ç»ƒå¾ªç¯
epochs = 200

def plot_confusion_matrix(y_true, y_pred, class_names, epoch, save_path):
    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))  # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)  # æ¯è¡Œå½’ä¸€åŒ–ä¸ºæ¯”ä¾‹

    # é¿å…é™¤ä»¥0å¯¼è‡´çš„ NaN
    cm_normalized = np.nan_to_num(cm_normalized)

    plt.figure(figsize=(8, 6))
    sns.heatmap(
        cm_normalized,
        annot=True,
        fmt=".2%",  # æ˜¾ç¤ºä¸ºç™¾åˆ†æ¯”
        cmap="Blues",
        xticklabels=class_names,
        yticklabels=class_names,
        cbar=True
    )
    plt.xlabel('Predicted labels')
    plt.ylabel('True labels')
    plt.title(f'Confusion Matrix (Normalized %) - Epoch {epoch}')
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()

# è®¡ç®—æ¯ä¸ªç±»åˆ«çš„è¯„ä¼°æŒ‡æ ‡
def calculate_metrics(preds, labels, num_classes=3):
    preds = preds.cpu().numpy().flatten()
    labels = labels.cpu().numpy().flatten()


    iou_list, precision_list, recall_list, dice_list, f1_list = [], [], [], [], []
    for cls in range(num_classes):
        pred_cls = (preds == cls)
        label_cls = (labels == cls)

        TP = np.logical_and(pred_cls, label_cls).sum()
        FP = np.logical_and(pred_cls, ~label_cls).sum()
        FN = np.logical_and(~pred_cls, label_cls).sum()

        union = np.logical_or(pred_cls, label_cls).sum()
        iou = TP / (union + 1e-6)
        precision = TP / (TP + FP + 1e-6)
        recall = TP / (TP + FN + 1e-6)
        dice = 2 * TP / (2 * TP + FP + FN + 1e-6)
        f1 = 2 * precision * recall / (precision + recall + 1e-6)

        iou_list.append(iou)
        precision_list.append(precision)
        recall_list.append(recall)
        dice_list.append(dice)
        f1_list.append(f1)

    return iou_list, precision_list, recall_list, dice_list, f1_list
def calculate_macro_average(iou, precision, recall, dice, f1):
    return (
        np.mean(iou),
        np.mean(precision),
        np.mean(recall),
        np.mean(dice),
        np.mean(f1)
    )

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

for epoch in range(epochs):
    start_time = time.time()
    model.train()
    train_loss = 0
    loop = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]', leave=False)
    for X, Y in loop:
        optimizer.zero_grad()
        X = X.to(device)  # è½¬ç§»åˆ° GPU
        Y = Y.to(device)  # è½¬ç§»åˆ° GPU
        X = X.reshape(-1, X.size(2), X.size(3), X.size(4), X.size(5))
        X = X.permute(0, 4, 1, 2, 3)
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()

        outputs = model(X)
        Y = Y.reshape(-1, Y.size(2), Y.size(3), Y.size(4), Y.size(5))
        Y = Y.permute(0, 4, 1, 2, 3)
        Y = Y.squeeze(1)  # å»æ‰å¤šä½™çš„é€šé“ç»´åº¦ï¼Œå˜ä¸º [batch_size, depth, height, width]
        Y = Y.long()

        loss = criterion(outputs, Y)
        loss.backward()
        torch.cuda.empty_cache() 
        optimizer.step()

        train_loss += loss.item()

    train_loss /= len(train_loader)
    # è®°å½•å½“å‰epochçš„è®­ç»ƒæ—¶é•¿
    epoch_time = time.time() - start_time  # è®¡ç®—è®­ç»ƒæ—¶é•¿
    print(f"Epoch {epoch + 1}/{epochs}, è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒæ—¶é•¿: {epoch_time:.2f}ç§’")
    param_count = count_parameters(model) / 1e6
    print(f"æ¨¡å‹å‚æ•°é‡: {param_count:.2f}M")
    # è®°å½•è®­ç»ƒæŸå¤±å’Œæ—¶é•¿åˆ°æ—¥å¿—æ–‡ä»¶
    with open("./log/SeismicTUMSAA/train-lossseismicTUMSAAæŸå¤±ä¼˜åŒ–.txt", mode='a') as file:
        file.write(f'{epoch + 1}\t{train_loss:.4f}\t{epoch_time:.2f}\n')
        file.flush()  # ç¡®ä¿æ•°æ®å†™å…¥æ–‡ä»¶     
    # åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œè¯„ä¼°
    model.eval()
    val_loss = 0
    val_accuracy = 0
    val_recall = np.zeros(3)
    val_precision = np.zeros(3)
    val_iou = np.zeros(3)
    val_dice = np.zeros(3)
    val_f1 = np.zeros(3)

    val_road_precision = 0  # æ²³é“çš„å®å¹³å‡
    val_road_recall = 0
    val_road_f1 = 0
    val_road_iou = 0
    val_road_dice = 0

    val_cave_precision = 0  # æº¶æ´çš„å®å¹³å‡
    val_cave_recall = 0
    val_cave_f1 = 0
    val_cave_iou = 0
    val_cave_dice = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for X, Y in valid_loader:
            X = X.to(device)
            Y = Y.to(device)
            X = X.reshape(-1, X.size(2), X.size(3), X.size(4), X.size(5))
            X = X.permute(0, 4, 1, 2, 3)
            outputs = model(X)
            Y = Y.reshape(-1, Y.size(2), Y.size(3), Y.size(4), Y.size(5))
            Y = Y.permute(0, 4, 1, 2, 3)
            # Y = Y.long()
            preds = torch.argmax(outputs, dim=1)  # [B, D, H, W]

            # Ground truth æ ‡ç­¾
            Y = Y.squeeze(1).long()               # [B, D, H, W]

            # æ·»åŠ åˆ°åˆ—è¡¨ä¸­
            all_preds.append(preds.cpu().numpy().flatten())
            all_labels.append(Y.cpu().numpy().flatten())
            # print("ğŸ“Š preds unique:", np.unique(all_preds, return_counts=True))
            # print("ğŸ“Š labels unique:", np.unique(all_labels, return_counts=True))
            loss = criterion(outputs, Y)
            val_loss += loss.item()
            # # preds = torch.argmax(outputs, dim=1)
            # all_preds.append(outputs.cpu().numpy())
            # all_labels.append(Y.cpu().numpy())

    val_loss /= len(valid_loader)

    all_preds_flat = np.concatenate(all_preds)
    all_labels_flat = np.concatenate(all_labels)

    # === è®¡ç®—æŒ‡æ ‡ ===
    iou_per_class, precision_per_class, recall_per_class, dice_per_class, f1_per_class = calculate_metrics(
        torch.tensor(all_preds_flat), torch.tensor(all_labels_flat), num_classes=3
    )
    macro_iou, macro_precision, macro_recall, macro_dice, macro_f1 = calculate_macro_average(
        iou_per_class, precision_per_class, recall_per_class, dice_per_class, f1_per_class
    )


    # === è¾“å‡ºè¯„ä¼°ç»“æœ ===
    print(f"Epoch {epoch+1}/{epochs}, éªŒè¯æŸå¤±: {val_loss:.4f}")
    print(f"[èƒŒæ™¯(ç±»åˆ«0)] ç²¾åº¦: {precision_per_class[0]:.4f}, å¬å›ç‡: {recall_per_class[0]:.4f}, F1: {f1_per_class[0]:.4f}, IoU: {iou_per_class[0]:.4f}")
    print(f"[æ²³é“(ç±»åˆ«1)] ç²¾åº¦: {precision_per_class[1]:.4f}, å¬å›ç‡: {recall_per_class[1]:.4f}, F1: {f1_per_class[1]:.4f}, IoU: {iou_per_class[1]:.4f}")
    print(f"[æº¶æ´(ç±»åˆ«2)] ç²¾åº¦: {precision_per_class[2]:.4f}, å¬å›ç‡: {recall_per_class[2]:.4f}, F1: {f1_per_class[2]:.4f}, IoU: {iou_per_class[2]:.4f}")
    print(f"[å®å¹³å‡] ç²¾åº¦: {macro_precision:.4f}, å¬å›ç‡: {macro_recall:.4f}, F1: {macro_f1:.4f}, IoU: {macro_iou:.4f}")

    # === ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶ ===
    with open("./log/SeismicTUMSAA/val-lossseismicTUMSAAæŸå¤±ä¼˜åŒ–.txt", mode='a') as file:
        file.write(f"\nEpoch {epoch+1}\n")
        file.write(f"éªŒè¯æŸå¤±: {val_loss:.4f}\n")
        file.write(f"[èƒŒæ™¯(ç±»åˆ«0)] ç²¾åº¦: {precision_per_class[0]:.4f}, å¬å›ç‡: {recall_per_class[0]:.4f}, F1: {f1_per_class[0]:.4f}, IoU: {iou_per_class[0]:.4f}\n")
        file.write(f"[æ²³é“(ç±»åˆ«1)] ç²¾åº¦: {precision_per_class[1]:.4f}, å¬å›ç‡: {recall_per_class[1]:.4f}, F1: {f1_per_class[1]:.4f}, IoU: {iou_per_class[1]:.4f}\n")
        file.write(f"[æº¶æ´(ç±»åˆ«2)] ç²¾åº¦: {precision_per_class[2]:.4f}, å¬å›ç‡: {recall_per_class[2]:.4f}, F1: {f1_per_class[2]:.4f}, IoU: {iou_per_class[2]:.4f}\n")
        file.write(f"[å®å¹³å‡] ç²¾åº¦: {macro_precision:.4f}, å¬å›ç‡: {macro_recall:.4f}, F1: {macro_f1:.4f}, IoU: {macro_iou:.4f}\n")
        file.flush()

    # è°ƒæ•´å­¦ä¹ ç‡
    scheduler.step(val_loss)
    # âœ… æ··æ·†çŸ©é˜µç»˜å›¾
    plot_confusion_matrix(
        y_true=all_labels_flat,
        y_pred=all_preds_flat,
        class_names=['èƒŒæ™¯', 'æ²³é“', 'æº¶æ´'],
        epoch=epoch + 1,
        save_path=f"./log/SeismicTUMSAA/SeismicTUMSAAæŸå¤±ä¼˜åŒ–_epoch{epoch+1}.png"
    )

    # ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
    save_checkpoint({
        'epoch': epoch + 1,
        'state_dict': model.state_dict(),
        'optimizer': optimizer.state_dict(),
    }, filename=f"/root/autodl-tmp/modelseismicTU-MSAA/SeismicTUMSAAæŸå¤±ä¼˜åŒ–.{epoch + 1:02d}.pth")

    print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')

writer.close() 
